{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib nbAgg\n",
    "%matplotlib nbAgg\n",
    "#>>>>>> Imports <<<<<<\n",
    "from sklearn import preprocessing\n",
    "import xarray as xr\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "#>>>>>> Variables <<<<<<\n",
    "# Step 1: Preparing data\n",
    "input_dir = \"/mount/studenten/arbeitsdaten-studenten1/alatrarm/DL-project/\" \n",
    "#names = ['data_prosody.train', 'data_logMel.train', 'data_prosody.valid', 'data_logMel.valid', 'data_prosody.test', \n",
    "#         'data_logMel.test']\n",
    "file_paths = [input_dir + \"data_prosody.train\", input_dir + \"data_logMel.train\", input_dir + \"data_prosody.valid\", \n",
    "              input_dir + \"data_logMel.valid\", input_dir + \"data_prosody.test\", input_dir + \"data_logMel.test\"]\n",
    "\n",
    "logMel_train_ids = []\n",
    "logMel_train_features = []\n",
    "logMel_train_labels = []\n",
    "logMel_valid_ids = []\n",
    "logMel_valid_features = []\n",
    "logMel_valid_labels = []\n",
    "logMel_test_ids = []\n",
    "logMel_test_features = []\n",
    "logMel_test_labels = []\n",
    "input_height = 750 # height = no. of rows/segments (750)\n",
    "input_width = 26 # this is the no. of cols (26 in logmel file)\n",
    "# Fix random seed for reproducibility\n",
    "seed = 42\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "#>>>>>> functions <<<<<<\n",
    "def unpickle_data(path):\n",
    "    '''returns 3 lists: data Ids, features, labels'''    \n",
    "    with open(path) as f_in:\n",
    "        ids = pickle.load(f_in)\n",
    "        features = pickle.load(f_in)\n",
    "        labels = pickle.load(f_in)\n",
    "        #print(len(ids), len(features), len(labels))\n",
    "        #print(\"{0}\\n{1}\\n{2}\".format(ids[10],features[10],labels[10]))\n",
    "        return ids, features, labels\n",
    "\n",
    "# 1.1 unpickle files\n",
    "logMel_file_paths = list(x for x in file_paths if \"logMel\" in x)\n",
    "#print(logMel_file_paths)\n",
    "logMel_train_ids,logMel_train_features,logMel_train_labels = unpickle_data(logMel_file_paths[0])\n",
    "logMel_valid_ids,logMel_valid_features,logMel_valid_labels = unpickle_data(logMel_file_paths[1])\n",
    "logMel_test_ids,logMel_test_features,logMel_test_labels = unpickle_data(logMel_file_paths[2])\n",
    "\n",
    "#1.2 Normalize Data\n",
    "train_scaler = preprocessing.Normalizer(norm='l2', copy=False).fit(logMel_train_features[0,:,:])\n",
    "valid_scaler = preprocessing.Normalizer(norm='l2', copy=False).fit(logMel_valid_features[0,:,:])\n",
    "test_scaler = preprocessing.Normalizer(norm='l2', copy=False).fit(logMel_test_features[0,:,:])\n",
    "train_scaler.transform(logMel_train_features[0,:,:])\n",
    "valid_scaler.transform(logMel_valid_features[0,:,:])\n",
    "test_scaler.transform(logMel_test_features[0,:,:])\n",
    "#print(logMel_train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.0 encode the string labels into numeric values    \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"angry\", \"happy\", \"sad\", \"neutral\"])\n",
    "logMel_train_encoded_labels = le.transform(logMel_train_labels)\n",
    "logMel_valid_encoded_labels = le.transform(logMel_valid_labels)\n",
    "\n",
    "# to decode these labels use the following command: example\n",
    "#results = list(le.inverse_transform([2, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.1 One hot encoding for the target vector\n",
    "from keras.utils import np_utils\n",
    "y_logMel_train_cat = np_utils.to_categorical(logMel_train_encoded_labels)\n",
    "y_logMel_valid_cat = np_utils.to_categorical(logMel_valid_encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531, 750, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "# 1.4 reshape data into 4D matrix (CNN takes 4D) \n",
    "# params\n",
    "input_channels = 1\n",
    "#X_train_4D = np.expand_dims(logMel_train_features, axis=-1)\n",
    "X_logMel_train_4D = logMel_train_features.reshape(logMel_train_features.shape[0], input_height, \n",
    "                                                  input_width , input_channels).astype('float32')\n",
    "print(X_logMel_train_4D.shape)\n",
    "X_logMel_valid_4D = logMel_valid_features.reshape(logMel_valid_features.shape[0], input_height, \n",
    "                                                  input_width , input_channels).astype('float32')\n",
    "X_logMel_test_4D = logMel_test_features.reshape(logMel_test_features.shape[0], input_height, \n",
    "                                                  input_width , input_channels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_1 (Model)              (None, 100)               1509300   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 1,509,704\n",
      "Trainable params: 1,509,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 2: define model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import optimizers\n",
    "\n",
    "# Paramters\n",
    "filters_l1 = 100\n",
    "filter_heights = [5,10]\n",
    "filter_width = input_width\n",
    "pool_height = 30\n",
    "pool_width = 1\n",
    "pool_stride = 3\n",
    "filter_stride = 3\n",
    "num_classes = 4\n",
    "loss_func = 'categorical_crossentropy' #'mse'\n",
    "#MSE is used for regression mostly (see ex3 and https://www.youtube.com/watch?v=IVVVjBSk9N0)\n",
    "optimizer_func = 'adam'\n",
    "conv_actv = 'relu'\n",
    "dropout_rate = 0.5\n",
    "img_path='network_image.png'\n",
    "\n",
    "#2.0 begin model design\n",
    "# Parallel layers code from KEras.pdf and here:\n",
    "# https://stackoverflow.com/questions/43151775/how-to-have-parallel-convolutional-layers-in-keras\n",
    "#2.1 input shape defined for our variant filter-width layer \n",
    "model_input = Input(shape=X_logMel_train_4D.shape[1:])\n",
    "conv_blocks = []\n",
    "for i in range(0,2):\n",
    "    # add a conv layer with the specified width \n",
    "    conv = Conv2D(filters=filters_l1, kernel_size=(filter_heights[i], filter_width), \n",
    "                  activation=conv_actv,strides=filter_stride)(model_input)\n",
    "    # add pooling \n",
    "    pooled = MaxPooling2D(pool_size=(pool_height, pool_width),strides =pool_stride)(conv)\n",
    "    # flatten results\n",
    "    pooled_flat = Flatten()(pooled)\n",
    "    conv_blocks.append(pooled_flat)\n",
    "if len(conv_blocks) > 1: # here we merge\n",
    "    cp_all = Concatenate()(conv_blocks)\n",
    "else:\n",
    "    cp_all = conv_blocks[0]   \n",
    "# 2.2 add dense layer to handle the output of the previous layer\n",
    "model_output = Dense(100, activation='relu')(cp_all)\n",
    "\n",
    "#2.3 define sub model (our convoluaitonal layer with various filter sizes)\n",
    "sub_model = Model(model_input, model_output)\n",
    "\n",
    "#2.4 define overall model\n",
    "model = Sequential()\n",
    "# 2.5 first convolutional layer\n",
    "model.add(sub_model)\n",
    "BatchNormalization(momentum=0.99)\n",
    "#2.6 add dropout\n",
    "model.add(Dropout(dropout_rate))\n",
    "BatchNormalization(momentum=0.99)\n",
    "#2.7 add dense (fully connected) layer for the softmax\n",
    "model.add(Dense(num_classes, activation='softmax'))  \n",
    "# note from slides: # params >> # training instances is bad\n",
    "model.summary()\n",
    "# 2.8 save structure of model\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file=img_path)\n",
    "\n",
    "#2.9 compile model\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=True)\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=False)\n",
    "optimizer_func = adam #sgd \n",
    "model.compile(loss=loss_func, optimizer=optimizer_func, metrics=['accuracy'])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5531 samples, validate on 5531 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 1.2493 - acc: 0.3918 - val_loss: 1.1269 - val_acc: 0.4826\n",
      "Epoch 2/100\n",
      " - 6s - loss: 1.1843 - acc: 0.4325 - val_loss: 1.0867 - val_acc: 0.5039\n",
      "Epoch 3/100\n",
      " - 5s - loss: 1.1578 - acc: 0.4493 - val_loss: 1.0700 - val_acc: 0.5005\n",
      "Epoch 4/100\n",
      " - 5s - loss: 1.1356 - acc: 0.4654 - val_loss: 1.0365 - val_acc: 0.5343\n",
      "Epoch 5/100\n",
      " - 5s - loss: 1.1123 - acc: 0.4739 - val_loss: 1.0322 - val_acc: 0.5406\n",
      "Epoch 6/100\n",
      " - 5s - loss: 1.1048 - acc: 0.4867 - val_loss: 1.0019 - val_acc: 0.5509\n",
      "Epoch 7/100\n",
      " - 5s - loss: 1.0845 - acc: 0.4994 - val_loss: 0.9855 - val_acc: 0.5766\n",
      "Epoch 8/100\n",
      " - 5s - loss: 1.0709 - acc: 0.4932 - val_loss: 0.9732 - val_acc: 0.5672\n",
      "Epoch 9/100\n",
      " - 5s - loss: 1.0490 - acc: 0.5149 - val_loss: 0.9662 - val_acc: 0.5946\n",
      "Epoch 10/100\n",
      " - 5s - loss: 1.0386 - acc: 0.5259 - val_loss: 0.9437 - val_acc: 0.5894\n",
      "Epoch 11/100\n",
      " - 5s - loss: 1.0285 - acc: 0.5265 - val_loss: 0.9232 - val_acc: 0.6138\n",
      "Epoch 12/100\n",
      " - 5s - loss: 1.0205 - acc: 0.5245 - val_loss: 0.9621 - val_acc: 0.5852\n",
      "Epoch 13/100\n",
      " - 5s - loss: 0.9981 - acc: 0.5415 - val_loss: 0.8886 - val_acc: 0.6160\n",
      "Epoch 14/100\n",
      " - 5s - loss: 1.0007 - acc: 0.5393 - val_loss: 0.8926 - val_acc: 0.6142\n",
      "Epoch 15/100\n",
      " - 5s - loss: 0.9801 - acc: 0.5554 - val_loss: 0.8792 - val_acc: 0.6377\n",
      "Epoch 16/100\n",
      " - 5s - loss: 0.9490 - acc: 0.5719 - val_loss: 0.8381 - val_acc: 0.6474\n",
      "Epoch 17/100\n",
      " - 5s - loss: 0.9317 - acc: 0.5758 - val_loss: 0.8510 - val_acc: 0.6257\n",
      "Epoch 18/100\n",
      " - 5s - loss: 0.9353 - acc: 0.5796 - val_loss: 0.8226 - val_acc: 0.6424\n",
      "Epoch 19/100\n",
      " - 5s - loss: 0.9177 - acc: 0.5858 - val_loss: 0.8044 - val_acc: 0.6579\n",
      "Epoch 20/100\n",
      " - 5s - loss: 0.8993 - acc: 0.5963 - val_loss: 0.7721 - val_acc: 0.6805\n",
      "Epoch 21/100\n",
      " - 5s - loss: 0.8743 - acc: 0.6062 - val_loss: 0.7835 - val_acc: 0.6823\n",
      "Epoch 22/100\n",
      " - 5s - loss: 0.8728 - acc: 0.6142 - val_loss: 0.7490 - val_acc: 0.7046\n",
      "Epoch 23/100\n",
      " - 5s - loss: 0.8584 - acc: 0.6136 - val_loss: 0.7498 - val_acc: 0.6916\n",
      "Epoch 24/100\n",
      " - 5s - loss: 0.8375 - acc: 0.6266 - val_loss: 0.7358 - val_acc: 0.7019\n",
      "Epoch 25/100\n",
      " - 5s - loss: 0.8231 - acc: 0.6319 - val_loss: 0.6945 - val_acc: 0.7161\n",
      "Epoch 26/100\n",
      " - 5s - loss: 0.8149 - acc: 0.6326 - val_loss: 0.6758 - val_acc: 0.7324\n",
      "Epoch 27/100\n",
      " - 5s - loss: 0.8068 - acc: 0.6453 - val_loss: 0.6566 - val_acc: 0.7476\n",
      "Epoch 28/100\n",
      " - 5s - loss: 0.7839 - acc: 0.6587 - val_loss: 0.6748 - val_acc: 0.7407\n",
      "Epoch 29/100\n",
      " - 5s - loss: 0.7762 - acc: 0.6576 - val_loss: 0.6472 - val_acc: 0.7481\n",
      "Epoch 30/100\n",
      " - 5s - loss: 0.7666 - acc: 0.6626 - val_loss: 0.6223 - val_acc: 0.7686\n",
      "Epoch 31/100\n",
      " - 5s - loss: 0.7500 - acc: 0.6722 - val_loss: 0.6256 - val_acc: 0.7315\n",
      "Epoch 32/100\n",
      " - 5s - loss: 0.7339 - acc: 0.6773 - val_loss: 0.6048 - val_acc: 0.7673\n",
      "Epoch 33/100\n",
      " - 5s - loss: 0.7243 - acc: 0.6829 - val_loss: 0.5749 - val_acc: 0.7897\n",
      "Epoch 34/100\n",
      " - 5s - loss: 0.7083 - acc: 0.6907 - val_loss: 0.5742 - val_acc: 0.7930\n",
      "Epoch 35/100\n",
      " - 5s - loss: 0.7088 - acc: 0.6977 - val_loss: 0.5657 - val_acc: 0.7780\n",
      "Epoch 36/100\n",
      " - 5s - loss: 0.6980 - acc: 0.6995 - val_loss: 0.5377 - val_acc: 0.8091\n",
      "Epoch 37/100\n",
      " - 5s - loss: 0.6748 - acc: 0.7152 - val_loss: 0.5359 - val_acc: 0.7980\n",
      "Epoch 38/100\n",
      " - 5s - loss: 0.6763 - acc: 0.7093 - val_loss: 0.5154 - val_acc: 0.8156\n",
      "Epoch 39/100\n",
      " - 5s - loss: 0.6579 - acc: 0.7203 - val_loss: 0.5040 - val_acc: 0.8170\n",
      "Epoch 40/100\n",
      " - 5s - loss: 0.6488 - acc: 0.7149 - val_loss: 0.5013 - val_acc: 0.8288\n",
      "Epoch 41/100\n",
      " - 5s - loss: 0.6213 - acc: 0.7301 - val_loss: 0.4837 - val_acc: 0.8389\n",
      "Epoch 42/100\n",
      " - 5s - loss: 0.6237 - acc: 0.7339 - val_loss: 0.4702 - val_acc: 0.8400\n",
      "Epoch 43/100\n",
      " - 5s - loss: 0.6211 - acc: 0.7351 - val_loss: 0.4719 - val_acc: 0.8304\n",
      "Epoch 44/100\n",
      " - 5s - loss: 0.6024 - acc: 0.7416 - val_loss: 0.4423 - val_acc: 0.8413\n",
      "Epoch 45/100\n",
      " - 5s - loss: 0.5971 - acc: 0.7429 - val_loss: 0.4405 - val_acc: 0.8387\n",
      "Epoch 46/100\n",
      " - 5s - loss: 0.5883 - acc: 0.7478 - val_loss: 0.4367 - val_acc: 0.8510\n",
      "Epoch 47/100\n",
      " - 5s - loss: 0.5846 - acc: 0.7483 - val_loss: 0.4182 - val_acc: 0.8566\n",
      "Epoch 48/100\n",
      " - 5s - loss: 0.5555 - acc: 0.7639 - val_loss: 0.4041 - val_acc: 0.8671\n",
      "Epoch 49/100\n",
      " - 5s - loss: 0.5646 - acc: 0.7642 - val_loss: 0.4019 - val_acc: 0.8608\n",
      "Epoch 50/100\n",
      " - 5s - loss: 0.5445 - acc: 0.7744 - val_loss: 0.3828 - val_acc: 0.8711\n",
      "Epoch 51/100\n",
      " - 5s - loss: 0.5432 - acc: 0.7679 - val_loss: 0.3717 - val_acc: 0.8794\n",
      "Epoch 52/100\n",
      " - 5s - loss: 0.5383 - acc: 0.7707 - val_loss: 0.3711 - val_acc: 0.8783\n",
      "Epoch 53/100\n",
      " - 5s - loss: 0.5351 - acc: 0.7740 - val_loss: 0.3870 - val_acc: 0.8592\n",
      "Epoch 54/100\n",
      " - 5s - loss: 0.5095 - acc: 0.7874 - val_loss: 0.3683 - val_acc: 0.8774\n",
      "Epoch 55/100\n",
      " - 5s - loss: 0.5159 - acc: 0.7825 - val_loss: 0.3455 - val_acc: 0.8910\n",
      "Epoch 56/100\n",
      " - 5s - loss: 0.5090 - acc: 0.7845 - val_loss: 0.3403 - val_acc: 0.8922\n",
      "Epoch 57/100\n",
      " - 5s - loss: 0.5086 - acc: 0.7924 - val_loss: 0.3450 - val_acc: 0.8921\n",
      "Epoch 58/100\n",
      " - 5s - loss: 0.4944 - acc: 0.7950 - val_loss: 0.3352 - val_acc: 0.8924\n",
      "Epoch 59/100\n",
      " - 5s - loss: 0.4827 - acc: 0.8011 - val_loss: 0.3146 - val_acc: 0.9118\n",
      "Epoch 60/100\n",
      " - 5s - loss: 0.4816 - acc: 0.8013 - val_loss: 0.3216 - val_acc: 0.9138\n",
      "Epoch 61/100\n",
      " - 5s - loss: 0.4726 - acc: 0.8138 - val_loss: 0.3114 - val_acc: 0.9051\n",
      "Epoch 62/100\n",
      " - 5s - loss: 0.4704 - acc: 0.7988 - val_loss: 0.3155 - val_acc: 0.9181\n",
      "Epoch 63/100\n",
      " - 5s - loss: 0.4695 - acc: 0.8080 - val_loss: 0.3042 - val_acc: 0.9076\n",
      "Epoch 64/100\n",
      " - 5s - loss: 0.4487 - acc: 0.8214 - val_loss: 0.3025 - val_acc: 0.9116\n",
      "Epoch 65/100\n",
      " - 5s - loss: 0.4496 - acc: 0.8188 - val_loss: 0.2860 - val_acc: 0.9210\n",
      "Epoch 66/100\n",
      " - 5s - loss: 0.4461 - acc: 0.8131 - val_loss: 0.2792 - val_acc: 0.9125\n",
      "Epoch 67/100\n",
      " - 5s - loss: 0.4406 - acc: 0.8232 - val_loss: 0.2731 - val_acc: 0.9280\n",
      "Epoch 68/100\n",
      " - 5s - loss: 0.4385 - acc: 0.8188 - val_loss: 0.2709 - val_acc: 0.9251\n",
      "Epoch 69/100\n",
      " - 5s - loss: 0.4400 - acc: 0.8201 - val_loss: 0.2858 - val_acc: 0.9286\n",
      "Epoch 70/100\n",
      " - 5s - loss: 0.4268 - acc: 0.8322 - val_loss: 0.2576 - val_acc: 0.9327\n",
      "Epoch 71/100\n",
      " - 5s - loss: 0.4225 - acc: 0.8300 - val_loss: 0.2488 - val_acc: 0.9318\n",
      "Epoch 72/100\n",
      " - 5s - loss: 0.4234 - acc: 0.8315 - val_loss: 0.2506 - val_acc: 0.9306\n",
      "Epoch 73/100\n",
      " - 5s - loss: 0.4197 - acc: 0.8319 - val_loss: 0.2545 - val_acc: 0.9277\n",
      "Epoch 74/100\n",
      " - 5s - loss: 0.4154 - acc: 0.8351 - val_loss: 0.2493 - val_acc: 0.9286\n",
      "Epoch 75/100\n",
      " - 5s - loss: 0.4145 - acc: 0.8333 - val_loss: 0.2504 - val_acc: 0.9382\n",
      "Epoch 76/100\n",
      " - 5s - loss: 0.4067 - acc: 0.8375 - val_loss: 0.2444 - val_acc: 0.9360\n",
      "Epoch 77/100\n",
      " - 5s - loss: 0.3970 - acc: 0.8407 - val_loss: 0.2309 - val_acc: 0.9411\n",
      "Epoch 78/100\n",
      " - 5s - loss: 0.3931 - acc: 0.8454 - val_loss: 0.2225 - val_acc: 0.9452\n",
      "Epoch 79/100\n",
      " - 5s - loss: 0.3839 - acc: 0.8517 - val_loss: 0.2262 - val_acc: 0.9407\n",
      "Epoch 80/100\n",
      " - 5s - loss: 0.3865 - acc: 0.8516 - val_loss: 0.2230 - val_acc: 0.9467\n",
      "Epoch 81/100\n",
      " - 5s - loss: 0.3735 - acc: 0.8550 - val_loss: 0.2288 - val_acc: 0.9358\n",
      "Epoch 82/100\n",
      " - 5s - loss: 0.3784 - acc: 0.8490 - val_loss: 0.2225 - val_acc: 0.9320\n",
      "Epoch 83/100\n",
      " - 5s - loss: 0.3773 - acc: 0.8523 - val_loss: 0.2144 - val_acc: 0.9492\n",
      "Epoch 84/100\n",
      " - 5s - loss: 0.3720 - acc: 0.8517 - val_loss: 0.2053 - val_acc: 0.9519\n",
      "Epoch 85/100\n",
      " - 5s - loss: 0.3672 - acc: 0.8541 - val_loss: 0.2010 - val_acc: 0.9557\n",
      "Epoch 86/100\n",
      " - 5s - loss: 0.3685 - acc: 0.8526 - val_loss: 0.2022 - val_acc: 0.9445\n",
      "Epoch 87/100\n",
      " - 5s - loss: 0.3584 - acc: 0.8559 - val_loss: 0.1983 - val_acc: 0.9595\n",
      "Epoch 88/100\n",
      " - 5s - loss: 0.3703 - acc: 0.8554 - val_loss: 0.1882 - val_acc: 0.9595\n",
      "Epoch 89/100\n",
      " - 5s - loss: 0.3492 - acc: 0.8646 - val_loss: 0.1875 - val_acc: 0.9561\n",
      "Epoch 90/100\n",
      " - 5s - loss: 0.3540 - acc: 0.8572 - val_loss: 0.1895 - val_acc: 0.9559\n",
      "Epoch 91/100\n",
      " - 5s - loss: 0.3441 - acc: 0.8678 - val_loss: 0.1807 - val_acc: 0.9577\n",
      "Epoch 92/100\n",
      " - 5s - loss: 0.3498 - acc: 0.8695 - val_loss: 0.1961 - val_acc: 0.9515\n",
      "Epoch 93/100\n",
      " - 5s - loss: 0.3425 - acc: 0.8642 - val_loss: 0.1756 - val_acc: 0.9624\n",
      "Epoch 94/100\n",
      " - 5s - loss: 0.3356 - acc: 0.8691 - val_loss: 0.1725 - val_acc: 0.9629\n",
      "Epoch 95/100\n",
      " - 5s - loss: 0.3220 - acc: 0.8752 - val_loss: 0.1751 - val_acc: 0.9600\n",
      "Epoch 96/100\n",
      " - 5s - loss: 0.3339 - acc: 0.8684 - val_loss: 0.1749 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      " - 5s - loss: 0.3315 - acc: 0.8715 - val_loss: 0.1751 - val_acc: 0.9604\n",
      "Epoch 98/100\n",
      " - 5s - loss: 0.3309 - acc: 0.8660 - val_loss: 0.1605 - val_acc: 0.9664\n",
      "Epoch 99/100\n",
      " - 5s - loss: 0.3224 - acc: 0.8801 - val_loss: 0.1715 - val_acc: 0.9649\n",
      "Epoch 100/100\n",
      " - 5s - loss: 0.3284 - acc: 0.8678 - val_loss: 0.1661 - val_acc: 0.9689\n",
      "Accuracy on dev set:96.8902549192%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: train model\n",
    "\n",
    "#3.0 use earlstopping to avoid overfitting with epochs\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# define path to save model\n",
    "model_path = './parallel_cnn_BN.h5'\n",
    "# prepare callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_acc', \n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        verbose=1),\n",
    "    ModelCheckpoint(model_path,\n",
    "        monitor='val_acc', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=0)\n",
    "]\n",
    "# Paramters\n",
    "total_epochs = 100\n",
    "batch_s = 32\n",
    "number_training_samples = X_logMel_train_4D.shape[1]\n",
    "\n",
    "#history = model.fit(X_logMel_train_4D, y_logMel_train_cat, validation_data=(X_logMel_valid_4D, y_logMel_valid_cat), epochs=total_epochs, batch_size=batch_s, verbose=2, shuffle=True)\n",
    "history = model.fit(X_logMel_train_4D, y_logMel_train_cat, validation_data=(X_logMel_train_4D, y_logMel_train_cat), epochs=total_epochs, batch_size=batch_s, verbose=2, shuffle=True)\n",
    "        \n",
    "# 3.1 evaluate model on dev set\n",
    "#score = model.evaluate(X_logMel_valid_4D, y_logMel_valid_cat, verbose=0)\n",
    "score = model.evaluate(X_logMel_train_4D, y_logMel_train_cat, verbose=0)\n",
    "#3.2 get score\n",
    "print(\"Accuracy on dev set:{0}%\".format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MetaKernel Python (Python 2)",
   "language": "python",
   "name": "python2-metakernel-python"
  },
  "language_info": {
   "file_extension": ".py",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
