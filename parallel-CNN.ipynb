{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib nbAgg\n",
    "%matplotlib nbAgg\n",
    "'''\n",
    "@author: Reem Alatrash\n",
	"@version: 1.0",
    "Python ver: 2.7\n",
	"=======================\n",
	"\n",
	"This script creates a convolutional neural network (CNN) with parallel convolution layers to classify the emotion embedded within speech signals (both scripted and improvised) into 1 of 4 emotions: happy, sad, angry, neutral.\n",
    "\n",
	"The system takes as input pre-processed features extracted from the speech subset of the Interactive Emotional Dyadic Motion Capture (IEMOCAP) database.\n",
	"The system produces 2 files:\n",
	"1. The model (parallel CNN), which is saved as 'parallel_cnn_BN.h5'.\n",
	"2. The predictions for both validation/dev and test sets in tab-seperated files. Each prediction file contains the IDs audio signals and their corresponding predicted class.\n",
	"\n",
	"Example:\n",
	"MSP-IMPROV-S08A-F05-S-FM02	happy\n",
	"\n",
	"Note(s): run in py 2.7 with the data extracted using py 2.7\n",
    "'''\n",
    "#>>>>>> Imports <<<<<<\n",
    "from sklearn import preprocessing\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#>>>>>> Variables <<<<<<\n",
    "# Step 1: Preparing data\n",
    "input_dir = \"./data/\" \n",
    "#names = ['data_prosody.train', 'data_logMel.train', 'data_prosody.valid', 'data_logMel.valid', 'data_prosody.test', \n",
    "#         'data_logMel.test']\n",
    "file_paths = [input_dir + \"data_prosody.train\", input_dir + \"data_logMel.train\", input_dir + \"data_prosody.valid\", \n",
    "              input_dir + \"data_logMel.valid\", input_dir + \"data_prosody.test\", input_dir + \"data_logMel.test\"]\n",
    "\n",
    "logMel_train_ids = []\n",
    "logMel_train_features = []\n",
    "logMel_train_labels = []\n",
    "logMel_valid_ids = []\n",
    "logMel_valid_features = []\n",
    "logMel_valid_labels = []\n",
    "logMel_test_ids = []\n",
    "logMel_test_features = []\n",
    "logMel_test_labels = []\n",
    "# Fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "#>>>>>> functions <<<<<<\n",
    "def unpickle_data(path):\n",
    "    '''returns 3 lists: data Ids, features, labels'''    \n",
    "    with open(path, 'rb') as f_in:\n",
    "        ids = pickle.load(f_in)\n",
    "        features = pickle.load(f_in)\n",
    "        labels = pickle.load(f_in)\n",
    "        #print(len(ids), len(features), len(labels))\n",
    "        #print(\"{0}\\n{1}\\n{2}\".format(ids[10],features[10],labels[10]))\n",
    "        return ids, features, labels\n",
    "\n",
    "# 1.1 unpickle files (load data)\n",
    "logMel_file_paths = list(x for x in file_paths if \"logMel\" in x)\n",
    "#print(logMel_file_paths)\n",
    "logMel_train_ids,logMel_train_features,logMel_train_labels = unpickle_data(logMel_file_paths[0])\n",
    "logMel_valid_ids,logMel_valid_features,logMel_valid_labels = unpickle_data(logMel_file_paths[1])\n",
    "logMel_test_ids,logMel_test_features,logMel_test_labels = unpickle_data(logMel_file_paths[2])\n",
    "\n",
    "#print(logMel_valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'neutral': 1738, 'happy': 1322, 'sad': 442, 'angry': 396})\n",
      "{'angry': 0.10159055926115956, 'happy': 0.3391482811698307, 'neutral': 0.44586967675731143, 'sad': 0.11339148281169831}\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "#### WARNING: Don't run this section until the model is stable ####\n",
    "###################################################################\n",
    "# 1.2 increase training set size using some examples from the validation set\n",
    "\n",
    "# 1.2.0 get distribution of validation set \n",
    "# we would like to perserve the distribution of data\n",
    "import collections\n",
    "# create a dictionary with the frequencies of the 4 labels\n",
    "counts_valid = collections.Counter(logMel_valid_labels)\n",
    "print counts_valid\n",
    "# create a dictionary with the percentages of the labels\n",
    "ratio_valid = {class_label : counts_valid[class_label]*1.0 / len(logMel_valid_labels) \n",
    "               for class_label in counts_valid.keys()}\n",
    "print ratio_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('old train size ', 5531)\n",
      "('new train size ', 6600)\n",
      "('examples to extract ', 1069)\n",
      "{'angry': (0.6003078501795756, 108.0), 'sad': (0.21549512570548757, 121.0), 'neutral': (0.6346844535659102, 476.0), 'happy': (0.5495125705490409, 362.0)}\n",
      "OrderedDict([('neutral', (0.6346844535659102, 476.0)), ('angry', (0.6003078501795756, 108.0)), ('happy', (0.5495125705490409, 362.0)), ('sad', (0.21549512570548757, 121.0))])\n",
      "{'angry': 108.0, 'sad': 121.0, 'neutral': 476.0, 'happy': 362.0}\n",
      "{'angry': 109.0, 'sad': 121.0, 'neutral': 477.0, 'happy': 362.0}\n",
      "{'angry': (0, 109), 'sad': (1718, 1839), 'neutral': (2160, 2637), 'happy': (396, 758)}\n",
      "MSP-IMPROV-S02N-F03-S-MF01\n",
      "MSP-IMPROV-S02N-F03-S-MF01\n"
     ]
    }
   ],
   "source": [
    "# 1.2.2 split the combined data according to the specified ratio\n",
    "# params\n",
    "old_train_size = len(logMel_train_ids)\n",
    "combined_size = old_train_size + len(logMel_valid_ids)\n",
    "split_ratio = 0.7 # Ex: 0.8 of combined data => 80% training, 20% dev\n",
    "new_train_size = int(combined_size * split_ratio)\n",
    "total_examples_to_extract = new_train_size - old_train_size\n",
    "\n",
    "print(\"old train size \", old_train_size)\n",
    "print(\"new train size \", new_train_size)\n",
    "print(\"examples to extract \", total_examples_to_extract)\n",
    "\n",
    "from math import modf, floor\n",
    "# 1.2.3 calculate no. of examples to extract for each class/label\n",
    "# split values into whole and decimal parts in order to perform Largest Remainder Method\n",
    "# https://stackoverflow.com/questions/13483430/how-to-make-rounded-percentages-add-up-to-100\n",
    "ratio_examples_modf = {class_label : modf(ratio_valid[class_label]* total_examples_to_extract)\n",
    "                  for class_label in ratio_valid.keys()}\n",
    "\n",
    "# sort dictionary by the value of the decimals\n",
    "ordered_by_decimals = collections.OrderedDict(sorted(ratio_examples_modf.items(), \n",
    "                                                     key=lambda t: t[1][0], reverse=True))\n",
    "print(ratio_examples_modf)\n",
    "print(ordered_by_decimals)\n",
    "\n",
    "# ratio_examples =  {'angry': 109.0, 'sad': 121.0, 'neutral': 477.0, 'happy': 362.0}\n",
    "ratio_examples = {class_label : floor(ratio_valid[class_label]* total_examples_to_extract)\n",
    "                  for class_label in ratio_valid.keys()}\n",
    "print(ratio_examples)\n",
    "# make sure the number of examples from ratios doesn't exceed the split ratio\n",
    "current_total = sum(ratio_examples.values()) \n",
    "\n",
    "diff_ex = total_examples_to_extract - current_total\n",
    "if diff_ex > 0:\n",
    "    # add 1 to the top diff items | e.g. diff = 3, add 1 to the top 3 items\n",
    "    for label in ordered_by_decimals.keys()[:int(diff_ex)]:\n",
    "        ratio_examples[label] += 1\n",
    "        \n",
    "print(ratio_examples)        \n",
    "\n",
    "\n",
    "# 1.2.3 Add examples to Training data set\n",
    "\n",
    "# ordered of the labels in the validation set\n",
    "label_order = [\"angry\", \"happy\", \"sad\", \"neutral\"]\n",
    "subset_start = 0\n",
    "# get the start and end ID for each label subset that needs to be extracted\n",
    "# {'angry': (0, 109), 'sad': (1718, 1839), 'neutral': (2160, 2637), 'happy': (396, 758)}\n",
    "valid_subsets_indixes = {}\n",
    "\n",
    "for label in label_order:\n",
    "    # add number of examples to be extracted the start index to get subset end\n",
    "    subset_end = subset_start + int(ratio_examples[label])\n",
    "    # set start and end of each label subset\n",
    "    valid_subsets_indixes[label] = (subset_start,subset_end)\n",
    "    # compute start index for next subset/label\n",
    "    subset_start += int(counts_valid[label])\n",
    "\n",
    "print(valid_subsets_indixes)\n",
    "\n",
    "# initialize the extended sets with the old train set and old valid set\n",
    "logmel_ext_train_ids = logMel_train_ids\n",
    "logmel_ext_train_features = logMel_train_features\n",
    "logmel_ext_train_labels = logMel_train_labels\n",
    "\n",
    "logmel_ext_valid_ids = logMel_valid_ids\n",
    "logmel_ext_valid_features = logMel_valid_features\n",
    "logmel_ext_valid_labels = logMel_valid_labels\n",
    "\n",
    "\n",
    "# change the data sets by extending training set and shrinking validation set\n",
    "for label in label_order:\n",
    "    label_range = valid_subsets_indixes[label]\n",
    "\n",
    "    # new train set: append samples from old validation set\n",
    "    logmel_ext_train_ids = np.append(logmel_ext_train_ids,logMel_valid_ids[label_range[0]:label_range[1]],axis=0)\n",
    "    logmel_ext_train_features = np.append(logmel_ext_train_features,\n",
    "                                          logMel_valid_features[label_range[0]:label_range[1]],axis=0)\n",
    "    logmel_ext_train_labels = np.append(logmel_ext_train_labels,\n",
    "                                        logMel_valid_labels[label_range[0]:label_range[1]],axis=0)     \n",
    "\n",
    "# new validation set: delete samples from old validation set\n",
    "for label in label_order[::-1]:\n",
    "    label_range = valid_subsets_indixes[label]\n",
    "    logmel_ext_valid_ids = np.delete(logmel_ext_valid_ids, range(label_range[0],label_range[1]),axis=0)\n",
    "    logmel_ext_valid_features = np.delete(logmel_ext_valid_features, range(label_range[0],label_range[1]),axis=0)\n",
    "    logmel_ext_valid_labels = np.delete(logmel_ext_valid_labels, range(label_range[0],label_range[1]),axis=0)  \n",
    "\n",
    "# sanity check    \n",
    "print(logmel_ext_valid_ids[0])\n",
    "print(logMel_valid_ids[109])\n",
    "\n",
    "# override the old variables with the new data\n",
    "logMel_train_ids = logmel_ext_train_ids\n",
    "logMel_train_features = logmel_ext_train_features\n",
    "logMel_train_labels = logmel_ext_train_labels\n",
    "\n",
    "logMel_valid_ids = logmel_ext_valid_ids\n",
    "logMel_valid_features = logmel_ext_valid_features\n",
    "logMel_valid_labels = logmel_ext_valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 Normalize data\n",
    "train_scaler = preprocessing.Normalizer(norm='l2', copy=False).fit(logMel_train_features[0,:,:])\n",
    "valid_scaler = preprocessing.Normalizer(norm='l2', copy=False).fit(logMel_valid_features[0,:,:])\n",
    "test_scaler = preprocessing.Normalizer(norm='l2', copy=False).fit(logMel_test_features[0,:,:])\n",
    "train_scaler.transform(logMel_train_features[0,:,:])\n",
    "valid_scaler.transform(logMel_valid_features[0,:,:])\n",
    "test_scaler.transform(logMel_test_features[0,:,:])\n",
    "#print(logMel_train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 encode the string labels into numeric values    \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"angry\", \"happy\", \"sad\", \"neutral\"])\n",
    "logMel_train_encoded_labels = le.transform(logMel_train_labels)\n",
    "logMel_valid_encoded_labels = le.transform(logMel_valid_labels)\n",
    "\n",
    "# to decode these labels use the following command: example\n",
    "#results = list(le.inverse_transform([2, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4.1 One hot encoding for the target vectors\n",
    "from keras.utils import np_utils\n",
    "y_logMel_train_cat = np_utils.to_categorical(logMel_train_encoded_labels)\n",
    "y_logMel_valid_cat = np_utils.to_categorical(logMel_valid_encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6600, 750, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "# 1.5 reshape data into 4D matrix (CNN takes 4D) by adding no. of input channels\n",
    "# for tensorflow no. of input channels is the last dimension (Theano has it as first)\n",
    "# params\n",
    "input_height = 750 # height = no. of rows/segments (750)\n",
    "input_width = 26 # this is the no. of cols (26 in logmel file)\n",
    "input_channels = 1\n",
    "#X_train_4D = np.expand_dims(logMel_train_features, axis=-1)\n",
    "X_logMel_train_4D = logMel_train_features.reshape(logMel_train_features.shape[0], input_height, \n",
    "                                                  input_width , input_channels).astype('float32')\n",
    "print(X_logMel_train_4D.shape)\n",
    "X_logMel_valid_4D = logMel_valid_features.reshape(logMel_valid_features.shape[0], input_height, \n",
    "                                                  input_width , input_channels).astype('float32')\n",
    "X_logMel_test_4D = logMel_test_features.reshape(logMel_test_features.shape[0], input_height, \n",
    "                                                input_width , input_channels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_0 shape: (?, 249, 1, 100)\n",
      "pooled_0 shape: (?, 74, 1, 100)\n",
      "conv_1 shape: (?, 247, 1, 100)\n",
      "pooled_1 shape: (?, 74, 1, 100)\n",
      "concatinated tensor shape: (?, 74, 1, 200)\n",
      "dense_1 tensor shape: (?, 74, 1, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_1 (Model)              (None, 7400)              59300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7400)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 29604     \n",
      "=================================================================\n",
      "Total params: 88,904\n",
      "Trainable params: 88,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 2: define model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import optimizers, metrics\n",
    "\n",
    "# Paramters\n",
    "filters_l1 = 100\n",
    "filter_heights = [5,10]\n",
    "filter_width = input_width\n",
    "pool_height = [30,26]\n",
    "pool_width = 1\n",
    "pool_stride = 3\n",
    "filter_stride = 3\n",
    "num_classes = 4\n",
    "loss_func = 'categorical_crossentropy' #'mse' # 'binary_crossentropy' \n",
    "#MSE is used for regression mostly (see ex3 and https://www.youtube.com/watch?v=IVVVjBSk9N0)\n",
    "optimizer_func = 'adam'\n",
    "conv_actv = 'relu'\n",
    "dropout_rate = 0.5 #0.25 gave best results\n",
    "img_path='network_image.png'\n",
    "metric =  [metrics.categorical_accuracy]#['accuracy']\n",
    "\n",
    "# 2.0 begin model design\n",
    "# Parallel layers code from Keras.pdf and here:\n",
    "# https://stackoverflow.com/questions/43151775/how-to-have-parallel-convolutional-layers-in-keras\n",
    "\n",
    "# 2.1 input shape defined for our variant filter-width layer \n",
    "model_input = Input(shape=X_logMel_train_4D.shape[1:])\n",
    "conv_blocks = []\n",
    "for i in range(0,2):\n",
    "    # add a conv layer with the specified width \n",
    "    conv = Conv2D(filters=filters_l1, kernel_size=(filter_heights[i], filter_width), \n",
    "                  activation=conv_actv,strides=filter_stride)(model_input)\n",
    "    print(\"conv_{} shape: {}\".format(i, conv.shape))\n",
    "    # add pooling \n",
    "    pooled = MaxPooling2D(pool_size=(pool_height[i], pool_width),strides =pool_stride)(conv)\n",
    "    print(\"pooled_{} shape: {}\".format(i, pooled.shape))\n",
    "    # flatten results\n",
    "    #pooled_flat = Flatten()(pooled)\n",
    "    conv_blocks.append(pooled)#(pooled_flat)\n",
    "if len(conv_blocks) > 1: # here we merge\n",
    "    cp_all = Concatenate(axis=-1)(conv_blocks)\n",
    "else:\n",
    "    cp_all = conv_blocks[0]\n",
    "    \n",
    "# 2.2 add dense layer to handle the output of the previous layer\n",
    "print(\"concatinated tensor shape: {}\".format(cp_all.shape))\n",
    "\n",
    "# Dense documentations flattens the input tensor by default if its rank is > 2 \n",
    "# https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L787\n",
    "den = Dense(100, activation='relu')(cp_all)\n",
    "print(\"dense_1 tensor shape: {}\".format(den.shape))\n",
    "\n",
    "#  The extra flattening below yields better results, I don't know why\n",
    "# Notes: \n",
    "# 1) the number of parameters in the sub model goes down from 1,509,300 to 59,300. However, for the softamx layer \n",
    "# the no. of params increases from 404 to 29,604 \n",
    "# 2) the model seems to predict anger and sadness more accuaretly with this change.\n",
    "model_output = Flatten()(den)\n",
    "\n",
    "#print(model_output.shape)\n",
    "\n",
    "# 2.3 define sub model (our convoluaitonal layer with various filter sizes)\n",
    "sub_model = Model(model_input, model_output)\n",
    "\n",
    "# 2.4 define overall model\n",
    "model = Sequential()\n",
    "# 2.5 first convolutional layer\n",
    "model.add(sub_model)\n",
    "\n",
    "BatchNormalization(momentum=0.99)\n",
    "# 2.6 add dropout\n",
    "model.add(Dropout(dropout_rate))\n",
    "BatchNormalization(momentum=0.99)\n",
    "\n",
    "# 2.7 add dense (fully connected) layer for the softmax\n",
    "model.add(Dense(num_classes, activation='softmax'))  \n",
    "# note from slides: # params >> # training instances is bad\n",
    "model.summary()\n",
    "# 2.8 save structure of model\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file=img_path)\n",
    "\n",
    "#2.9 compile model\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=True)\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=False)\n",
    "optimizer_func = adam #sgd \n",
    "model.compile(loss=loss_func, optimizer=optimizer_func, metrics=metric)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6600 samples, validate on 2829 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 1.2212 - categorical_accuracy: 0.4361 - val_loss: 1.2003 - val_categorical_accuracy: 0.4507\n",
      "Epoch 2/50\n",
      " - 4s - loss: 1.1520 - categorical_accuracy: 0.4712 - val_loss: 1.2845 - val_categorical_accuracy: 0.4224\n",
      "Epoch 3/50\n",
      " - 4s - loss: 1.0867 - categorical_accuracy: 0.5105 - val_loss: 1.1499 - val_categorical_accuracy: 0.4698\n",
      "Epoch 4/50\n",
      " - 4s - loss: 1.0715 - categorical_accuracy: 0.5197 - val_loss: 1.1547 - val_categorical_accuracy: 0.4751\n",
      "Epoch 5/50\n",
      " - 4s - loss: 1.0306 - categorical_accuracy: 0.5432 - val_loss: 1.1240 - val_categorical_accuracy: 0.4839\n",
      "Epoch 6/50\n",
      " - 4s - loss: 1.0163 - categorical_accuracy: 0.5503 - val_loss: 1.1808 - val_categorical_accuracy: 0.4920\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.9822 - categorical_accuracy: 0.5703 - val_loss: 1.1260 - val_categorical_accuracy: 0.5034\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.9598 - categorical_accuracy: 0.5856 - val_loss: 1.1416 - val_categorical_accuracy: 0.5002\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.9416 - categorical_accuracy: 0.5941 - val_loss: 1.1177 - val_categorical_accuracy: 0.5133\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.9153 - categorical_accuracy: 0.6065 - val_loss: 1.1231 - val_categorical_accuracy: 0.5274\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.8765 - categorical_accuracy: 0.6326 - val_loss: 1.1409 - val_categorical_accuracy: 0.5175\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.8589 - categorical_accuracy: 0.6376 - val_loss: 1.1505 - val_categorical_accuracy: 0.5277\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.8421 - categorical_accuracy: 0.6441 - val_loss: 1.1514 - val_categorical_accuracy: 0.5437\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.8174 - categorical_accuracy: 0.6580 - val_loss: 1.1690 - val_categorical_accuracy: 0.5154\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.8003 - categorical_accuracy: 0.6767 - val_loss: 1.1558 - val_categorical_accuracy: 0.5338\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.7764 - categorical_accuracy: 0.6798 - val_loss: 1.1366 - val_categorical_accuracy: 0.5263\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.7522 - categorical_accuracy: 0.6833 - val_loss: 1.1352 - val_categorical_accuracy: 0.5327\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.7368 - categorical_accuracy: 0.6994 - val_loss: 1.1319 - val_categorical_accuracy: 0.5203\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.7240 - categorical_accuracy: 0.7048 - val_loss: 1.1593 - val_categorical_accuracy: 0.5422\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.7107 - categorical_accuracy: 0.7102 - val_loss: 1.1699 - val_categorical_accuracy: 0.5224\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.6899 - categorical_accuracy: 0.7195 - val_loss: 1.1426 - val_categorical_accuracy: 0.5341\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.6807 - categorical_accuracy: 0.7276 - val_loss: 1.1498 - val_categorical_accuracy: 0.5306\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.6608 - categorical_accuracy: 0.7338 - val_loss: 1.1737 - val_categorical_accuracy: 0.5186\n",
      "Epoch 24/50\n",
      " - 4s - loss: 0.6523 - categorical_accuracy: 0.7406 - val_loss: 1.1909 - val_categorical_accuracy: 0.5210\n",
      "Epoch 25/50\n",
      " - 4s - loss: 0.6411 - categorical_accuracy: 0.7392 - val_loss: 1.2004 - val_categorical_accuracy: 0.5299\n",
      "Epoch 26/50\n",
      " - 4s - loss: 0.6243 - categorical_accuracy: 0.7520 - val_loss: 1.1758 - val_categorical_accuracy: 0.5299\n",
      "Epoch 27/50\n",
      " - 4s - loss: 0.6105 - categorical_accuracy: 0.7615 - val_loss: 1.1995 - val_categorical_accuracy: 0.5394\n",
      "Epoch 28/50\n",
      " - 4s - loss: 0.6059 - categorical_accuracy: 0.7609 - val_loss: 1.1755 - val_categorical_accuracy: 0.5327\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.5907 - categorical_accuracy: 0.7614 - val_loss: 1.1944 - val_categorical_accuracy: 0.5256\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.5860 - categorical_accuracy: 0.7714 - val_loss: 1.1626 - val_categorical_accuracy: 0.5376\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.5732 - categorical_accuracy: 0.7730 - val_loss: 1.1764 - val_categorical_accuracy: 0.5437\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.5672 - categorical_accuracy: 0.7773 - val_loss: 1.1925 - val_categorical_accuracy: 0.5338\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.5521 - categorical_accuracy: 0.7865 - val_loss: 1.1946 - val_categorical_accuracy: 0.5447\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.5563 - categorical_accuracy: 0.7789 - val_loss: 1.2302 - val_categorical_accuracy: 0.5507\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.5347 - categorical_accuracy: 0.7908 - val_loss: 1.1894 - val_categorical_accuracy: 0.5514\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.5352 - categorical_accuracy: 0.7930 - val_loss: 1.2110 - val_categorical_accuracy: 0.5422\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.5187 - categorical_accuracy: 0.8002 - val_loss: 1.2113 - val_categorical_accuracy: 0.5369\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.5187 - categorical_accuracy: 0.8027 - val_loss: 1.2310 - val_categorical_accuracy: 0.5486\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.5131 - categorical_accuracy: 0.7983 - val_loss: 1.1966 - val_categorical_accuracy: 0.5497\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.5029 - categorical_accuracy: 0.8068 - val_loss: 1.1951 - val_categorical_accuracy: 0.5500\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.5010 - categorical_accuracy: 0.8065 - val_loss: 1.2083 - val_categorical_accuracy: 0.5486\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.4912 - categorical_accuracy: 0.8138 - val_loss: 1.2349 - val_categorical_accuracy: 0.5373\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.4826 - categorical_accuracy: 0.8150 - val_loss: 1.2421 - val_categorical_accuracy: 0.5384\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.4671 - categorical_accuracy: 0.8230 - val_loss: 1.2327 - val_categorical_accuracy: 0.5426\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.4664 - categorical_accuracy: 0.8250 - val_loss: 1.2090 - val_categorical_accuracy: 0.5440\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.4536 - categorical_accuracy: 0.8302 - val_loss: 1.2305 - val_categorical_accuracy: 0.5461\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.4601 - categorical_accuracy: 0.8232 - val_loss: 1.2374 - val_categorical_accuracy: 0.5405\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.4545 - categorical_accuracy: 0.8276 - val_loss: 1.2575 - val_categorical_accuracy: 0.5387\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.4426 - categorical_accuracy: 0.8382 - val_loss: 1.2246 - val_categorical_accuracy: 0.5461\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.4327 - categorical_accuracy: 0.8394 - val_loss: 1.2511 - val_categorical_accuracy: 0.5429\n",
      "(?, ?)\n",
      "Accuracy on dev set:54.294803826%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: train model\n",
    "\n",
    "# define path to save best model\n",
    "model_path = './parallel_cnn_BN.h5'\n",
    "\n",
    "# 3.0 prepare callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "## use earlstopping to avoid overfitting with epochs\n",
    "early_stop = EarlyStopping(monitor='categorical_accuracy', patience=15, mode='max', verbose=1)\n",
    "# create a checkpoint to save the best model based on validation loss\n",
    "acc_chkpoint = ModelCheckpoint(model_path, monitor='val_categorical_accuracy', \n",
    "                                save_best_only=True, mode='max',verbose=0)\n",
    "callbacks = [acc_chkpoint,early_stop]\n",
    "# Paramters\n",
    "total_epochs = 50\n",
    "batch_s = 32\n",
    "number_training_samples = X_logMel_train_4D.shape[1]\n",
    "\n",
    "history = model.fit(X_logMel_train_4D, y_logMel_train_cat, validation_data=(X_logMel_valid_4D, y_logMel_valid_cat),\n",
    "                    epochs=total_epochs, batch_size=batch_s, verbose=2, shuffle=True,callbacks = callbacks)\n",
    "#history = model.fit(X_logMel_train_4D, y_logMel_train_cat, validation_data=(X_logMel_train_4D, y_logMel_train_cat), \n",
    "#                    epochs=total_epochs, batch_size=batch_s, verbose=2, shuffle=True)\n",
    "        \n",
    "# 3.1 evaluate model on dev set\n",
    "score = model.evaluate(X_logMel_valid_4D, y_logMel_valid_cat, verbose=0)\n",
    "#score = model.evaluate(X_logMel_train_4D, y_logMel_train_cat, verbose=0)\n",
    "\n",
    "print(model_output.shape)\n",
    "\n",
    "#3.2 get score\n",
    "print(\"Accuracy on dev set:{0}%\".format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:(3900, 4)\n",
      "[0.03733657 0.19391112 0.71529007 0.0534622 ]\n",
      "Prediction shape:(3900,)\n",
      "ids and classes lengths match\n",
      "MSP-IMPROV-S08A-F05-S-FM01\t2\n"
     ]
    }
   ],
   "source": [
    "# Step 4 Use model to predict\n",
    "# 4.1 Get Probability distribution over the classes shape:(elements, classes)\n",
    "prediction = model.predict(X_logMel_test_4D)\n",
    "\n",
    "print(\"Prediction shape:{0}\".format(prediction.shape))\n",
    "\n",
    "print prediction[0]\n",
    "\n",
    "# 4.2 Get the predictions list\n",
    "dev_predicted_classes = model.predict(X_logMel_valid_4D).argmax(axis=1)\n",
    "predicted_classes = model.predict(X_logMel_test_4D).argmax(axis=1)\n",
    "print(\"Prediction shape:{0}\".format(predicted_classes.shape))\n",
    "if len(logMel_test_ids) == len(predicted_classes):\n",
    "    print(\"ids and classes lengths match\")\n",
    "print(\"{0}\\t{1}\".format(logMel_test_ids[0],predicted_classes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# 4.3 Map predictions to ids and save to file\n",
    "results = list(le.inverse_transform(predicted_classes))\n",
    "dev_results = list(le.inverse_transform(predicted_classes))\n",
    "#4.4 save results to file\n",
    "\n",
    "with open(\"predictions_parallel_CNN_feb_8_dev.txt\", 'wb+') as f_out:\n",
    "    for idx in range(0,len(logMel_valid_ids)):\n",
    "        f_out.write(\"{0}\\t{1}\\n\".format(logMel_valid_ids[idx],dev_results[idx]))\n",
    "print(\"done\") \n",
    "\n",
    "with open(\"predictions_parallel_CNN_feb_8.txt\", 'wb+') as f_out:\n",
    "    for idx in range(0,len(logMel_test_ids)):\n",
    "        f_out.write(\"{0}\\t{1}\\n\".format(logMel_test_ids[idx],results[idx]))\n",
    "print(\"done\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'neutral': 1261, 'happy': 960, 'sad': 321, 'angry': 287})\n",
      "--------------------\n",
      "Correct Validation predictions: 1448\n",
      "Valdiation Accuracy: 0.511841640156\n",
      "--------------------\n",
      "Angry acc: 0.240418118467\n",
      "Happy acc: 0.495833333333\n",
      "Sad acc: 0.0498442367601\n",
      "Neutral acc: 0.70340999207\n"
     ]
    }
   ],
   "source": [
    "# Step 5 Manually check the results of the dev prediction\n",
    "# params\n",
    "correct = 0\n",
    "\n",
    "counts_valid = collections.Counter(logMel_valid_labels)\n",
    "print(counts_valid)\n",
    "\n",
    "c_pred_ang = 0\n",
    "c_pred_hap = 0\n",
    "c_pred_sad = 0\n",
    "c_pred_neu = 0\n",
    "\n",
    "# get count of correct label w.r.t. label\n",
    "def count_labels_by_cat(correct_label):\n",
    "    #print(correct_label)\n",
    "    global c_pred_ang\n",
    "    global c_pred_hap\n",
    "    global c_pred_sad\n",
    "    global c_pred_neu\n",
    "    if correct_label == \"happy\":\n",
    "        c_pred_hap += 1\n",
    "    elif correct_label == \"angry\":\n",
    "        c_pred_ang += 1\n",
    "    elif correct_label == \"sad\":\n",
    "        c_pred_sad += 1\n",
    "    else:\n",
    "        c_pred_neu += 1\n",
    "\n",
    "for idx in range(0,len(logMel_valid_ids)):\n",
    "    if dev_results[idx] == logMel_valid_labels[idx]:\n",
    "        correct += 1\n",
    "        count_labels_by_cat(dev_results[idx])\n",
    "print(\"--------------------\")\n",
    "print \"Correct Validation predictions:\", correct\n",
    "print \"Valdiation Accuracy:\", (correct*1.0)/len(logMel_valid_labels)\n",
    "print(\"--------------------\")\n",
    "print \"Angry acc:\", (c_pred_ang*1.0)/counts_valid['angry']\n",
    "print \"Happy acc:\", (c_pred_hap*1.0)/counts_valid['happy']\n",
    "print \"Sad acc:\", (c_pred_sad*1.0)/counts_valid['sad']\n",
    "print \"Neutral acc:\", (c_pred_neu*1.0)/counts_valid['neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MetaKernel Python (Python 2)",
   "language": "python",
   "name": "python2-metakernel-python"
  },
  "language_info": {
   "file_extension": ".py",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
